{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4c5d84",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b48558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextPreProc():\n",
    "    def __init__(self, stopwords_path = \"./stopwords-fa.txt\"):\n",
    "        try:\n",
    "            with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "                self.stopwords = set(f.read().splitlines())\n",
    "        except FileNotFoundError:\n",
    "            self.stopwords = set()\n",
    "\n",
    "    def replace_persian_char(self , text):\n",
    "        replacements = {'ÙŠ': 'ÛŒ', 'Ùƒ': 'Ú©'}\n",
    "        for arab, fa in replacements.items():\n",
    "            text = text.replace(arab, fa)\n",
    "        return text\n",
    "    \n",
    "    def convert_numbers_to_persian(self , text):\n",
    "        text = text.replace('0', 'Û°')\n",
    "        text = text.replace('1', 'Û±')\n",
    "        text = text.replace('2', 'Û²')\n",
    "        text = text.replace('3', 'Û³')\n",
    "        text = text.replace('4', 'Û´')\n",
    "        text = text.replace('5', 'Ûµ')\n",
    "        text = text.replace('6', 'Û¶')\n",
    "        text = text.replace('7', 'Û·')\n",
    "        text = text.replace('8', 'Û¸')\n",
    "        text = text.replace('9', 'Û¹')\n",
    "        return text\n",
    "    \n",
    "    def remove_links_and_tags(self , text):\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "        text = re.sub(r'RT', '', text)\n",
    "        text = re.sub(r'[\\U00010000-\\U0010ffff]', '', text)\n",
    "        return text\n",
    "\n",
    "    def remove_whitespaces(self , text):\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def remove_persian_stopwords(self , text):\n",
    "        if self.stopwords:\n",
    "            text = ' '.join([word for word in text.split() if word not in self.stopwords])\n",
    "        return text\n",
    "\n",
    "    def process_text(self , text):\n",
    "        text = self.replace_persian_char(text)\n",
    "        text = self.convert_numbers_to_persian(text)\n",
    "        text = self.remove_links_and_tags(text)\n",
    "        text = self.remove_whitespaces(text)\n",
    "        text = self.remove_persian_stopwords(text)\n",
    "        return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35287949",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ØºØ°Ø§Ø§Ø§Ø§Ø´ Ø§ØµÙ„Ø§ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯Ø¯Ø¯ ðŸ˜¡ ... Ù¾ÛŒÚ©Ø´ Ø³Ø±ÛŒØ¹ Ø¨ÙˆØ¯ @snappfood #Ø¨Ø¯ ... 1000 ØªÙˆÙ…Ù†! :))))\"\n",
    "process = TextPreProc()\n",
    "\n",
    "print(process.process_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e549b",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78709bc",
   "metadata": {},
   "source": [
    "## load fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b57496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"cc.fa.300.bin\")\n",
    "\n",
    "print(model.get_dimension())  \n",
    "print(model.get_word_vector(\"ØºØ°Ø§\").shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2059f2",
   "metadata": {},
   "source": [
    "## Determining sentence lengthâ€â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\n",
    "    \"./snappfood/train.csv\",\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "\n",
    "sentence_lengths = df_train[\"comment\"].apply(\n",
    "    lambda x: len(x.split())\n",
    ")\n",
    "\n",
    "print(sentence_lengths.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [90, 95, 99]:\n",
    "    print(f\"{p}th percentile:\", np.percentile(sentence_lengths, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "FAST_TEXT_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0cd465",
   "metadata": {},
   "source": [
    "## Embedding operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc079fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder():\n",
    "    def __init__(self, model, train_dataset: pd.DataFrame, max_length: int = MAX_LEN, dim: int = FAST_TEXT_DIM , stopwords_path = \"./stopwords-fa.txt\"):\n",
    "        self.model = model\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.max_length = max_length\n",
    "        self.dim = dim\n",
    "        self.preprocessor = TextPreProc(stopwords_path)\n",
    "\n",
    "\n",
    "    def embed_sentence(self, text: str) -> np.ndarray:\n",
    "        text = self.preprocessor.process_text(text)\n",
    "        words = text.split()\n",
    "\n",
    "        embeddings = []\n",
    "\n",
    "        # truncation\n",
    "        for w in words[:self.max_length]:\n",
    "            embeddings.append(self.model.get_word_vector(w))\n",
    "        \n",
    "        #padding\n",
    "        while len(embeddings) < self.max_length:\n",
    "            embeddings.append(np.zeros(self.dim))\n",
    "        \n",
    "        # transpose matrix needed for Conv1d\n",
    "        return np.array(embeddings).T\n",
    "\n",
    "    def process_dataset(self):\n",
    "\n",
    "        embedded_sentences = []\n",
    "\n",
    "        for text in self.train_dataset[\"comment\"]:\n",
    "            emb = self.embed_sentence(text)\n",
    "            embedded_sentences.append(emb)\n",
    "        \n",
    "        return np.stack(embedded_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(model, df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d9ecc",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e366f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataframe, embedder):\n",
    "        self.dataframe = dataframe\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row['comment']\n",
    "        label = row['label_id']\n",
    "\n",
    "        input_numpy = self.embedder.embed_sentence(text)\n",
    "        input_tensor = torch.tensor(input_numpy, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return input_tensor, label_tensor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SentimentDataset(dataframe=df_train, embedder=embedder)\n",
    "\n",
    "df_dev = pd.read_csv(\n",
    "    \"./snappfood/dev.csv\",\n",
    "    sep=\"\\t\"\n",
    ")\n",
    "dev_dataset = SentimentDataset(dataframe=df_dev, embedder=embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4157bf",
   "metadata": {},
   "source": [
    "# CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad41c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim = FAST_TEXT_DIM, hidden_dim = 128 , dropout_prob=0.5):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        linier_dim = int((MAX_LEN / 2) * hidden_dim)\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(linier_dim , 2)\n",
    "\n",
    "    def forward(self , x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = CNNClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266764b",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4396fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, train_loader, val_loader, loss_module, num_epochs=10, device='cpu'):\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(\"Start training ...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # dropout activation\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "        for inputs, labels in loop:\n",
    "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            ## Step 2: Run the model on the input data\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            ## Step 3: Calculate the loss\n",
    "            loss = loss_module(outputs, labels)\n",
    "\n",
    "            ## Step 4: Perform backpropagation\n",
    "            # Before calculating the gradients, we need to ensure that they are all zero.\n",
    "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            ## Step 5: Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            loop.set_postfix(loss=loss.item()) #update Progress bar\n",
    "\n",
    "        model.eval()\n",
    "        total_correct_pred = 0\n",
    "        total_data = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                total_correct_pred += (preds == labels).sum().item()\n",
    "                total_data += labels.size(0)\n",
    "\n",
    "            accuracy = (total_correct_pred / total_data) * 100\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_weights = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    print(f\"\\nTraining Finished. Best Validation Accuracy: {best_accuracy:.2f}%\")\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0cec54",
   "metadata": {},
   "source": [
    "# Finding hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ec143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "dropout_rates  = [0.3, 0.5]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for do in dropout_rates:\n",
    "        model = CNNClassifier(dropout_prob=do)\n",
    "\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        trained_model = train_model(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=dev_loader,\n",
    "            loss_module=loss,\n",
    "            num_epochs=10,\n",
    "        )\n",
    "\n",
    "        print(f\"Done with LR={lr}, DO={do}\")\n",
    "\n",
    "print(\"finish\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2586861",
   "metadata": {},
   "source": [
    "# Model training using specified hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = CNNClassifier(dropout_prob=0.3)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=dev_loader,\n",
    "    loss_module=loss,\n",
    "    num_epochs=30,\n",
    ")\n",
    "\n",
    "# model.load_state_dict(torch.load(\"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ec1f67",
   "metadata": {},
   "source": [
    "# Evaluation and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec02265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader , device='cpu'):\n",
    "    model.eval()\n",
    "    total_correct_pred = 0\n",
    "    total_data = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_correct_pred += (preds == labels).sum().item()\n",
    "            total_data += labels.size(0)\n",
    "\n",
    "    accuracy = (total_correct_pred / total_data) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fba025",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "df_test = pd.read_csv(\"./snappfood/test.csv\", sep=\"\\t\")\n",
    "test_dataset = SentimentDataset(dataframe=df_test, embedder=embedder)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Accuracy on test set: {evaluate(trained_model, test_loader):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
