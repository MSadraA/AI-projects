{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaSBweZwUuOk"
   },
   "source": [
    "# ğŸ¤– AI, CA3, Machine Learning ğŸ“š  \n",
    "\n",
    "* **Name** : Mohammad Sadra ğŸ–Š  \n",
    "* **Last Name** :  Abbasi ğŸ“  \n",
    "* **SID** : 810101469 ğŸ†”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "du8DTCJYUuOr"
   },
   "source": [
    "# IMPORT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and special characters (keep only a-z and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize (split by space)\n",
    "    words = text.split()\n",
    "    # Remove stopwords & Lemmatize\n",
    "    clean_words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n",
    "    return \" \".join(clean_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocab and vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class Vectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = None\n",
    "        self.count_vectorizer = None\n",
    "\n",
    "    def build_vocab(self, cleaned_set):\n",
    "        self.count_vectorizer = CountVectorizer(min_df=1)\n",
    "        \n",
    "        self.count_vectorizer.fit(cleaned_set)\n",
    "        \n",
    "        self.vocab = self.count_vectorizer.vocabulary_\n",
    "\n",
    "        return self.vocab\n",
    "    \n",
    "    def create_vectors(self, cleaned_set):\n",
    "        self.vocab = self.build_vocab(cleaned_set)\n",
    "        \n",
    "        if self.vocab is None:\n",
    "            raise ValueError(\"Vocabulary is empty.\")\n",
    "            \n",
    "        X_counts = self.count_vectorizer.transform(cleaned_set)\n",
    "        \n",
    "        binary_gen = CountVectorizer(vocabulary=self.vocab, binary=True)\n",
    "        X_binary = binary_gen.transform(cleaned_set)\n",
    "        \n",
    "        return X_counts, X_binary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "class TrainModel:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "\n",
    "    def train_models(self, X_train_counts, X_train_binary, y_train):\n",
    "        mnb = MultinomialNB()\n",
    "        mnb.fit(X_train_counts, y_train)\n",
    "        self.models['MultinomialNB'] = (mnb, 'counts')\n",
    "\n",
    "        bnb = BernoulliNB()\n",
    "        bnb.fit(X_train_binary, y_train)\n",
    "        self.models['BernoulliNB'] = (bnb, 'binary')\n",
    "\n",
    "        dt = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "        dt.fit(X_train_counts, y_train)\n",
    "        self.models['DecisionTree'] = (dt, 'counts')\n",
    "\n",
    "    def evaluate_models(self, X_val_counts, X_val_binary, y_val):\n",
    "        results_list = []\n",
    "\n",
    "        for name, (model, data_type) in self.models.items():\n",
    "            if data_type == 'counts':\n",
    "                X_test = X_val_counts\n",
    "            elif data_type == 'binary':\n",
    "                X_test = X_val_binary\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            metrics = {\n",
    "                \"Model\": name,\n",
    "                \"Accuracy\": accuracy_score(y_val, y_pred),\n",
    "                \"Precision\": precision_score(y_val, y_pred, zero_division=0),\n",
    "                \"Recall\": recall_score(y_val, y_pred, zero_division=0),\n",
    "                \"F1 Score\": f1_score(y_val, y_pred, zero_division=0)\n",
    "            }\n",
    "            results_list.append(metrics)\n",
    "\n",
    "            cm = confusion_matrix(y_val, y_pred)\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Spam', 'Important'])\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(4, 4))\n",
    "            disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)\n",
    "            plt.title(f\"Confusion Matrix\\n{name}\")\n",
    "            plt.show()\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        return pd.DataFrame(results_list)\n",
    "    \n",
    "    def generate_best_submission(self, X_test, message_texts, output_folder, model_key='MultinomialNB' , name = \"submission.csv\"):\n",
    "        if model_key not in self.models:\n",
    "            raise ValueError(f\"Model '{model_key}' not found!\")\n",
    "        \n",
    "        model, _ = self.models[model_key]\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        submission = pd.DataFrame({\n",
    "            'message_body': message_texts.values,\n",
    "            'prediction': predictions\n",
    "        })\n",
    "        \n",
    "        file_path = os.path.join(output_folder, name)\n",
    "        submission.to_csv(file_path, index=False)\n",
    "        \n",
    "        print(f\"File saved to: {file_path}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('./part2_dataset/part2_dataset.csv')\n",
    "\n",
    "df['clean_text'] = df['message_body'].apply(clean_text)\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "X_counts, X_binary = vectorizer.create_vectors(df['clean_text'])\n",
    "y = df['is_important']\n",
    "\n",
    "X_train_c, X_val_c, y_train, y_val = train_test_split(\n",
    "    X_counts, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "X_train_b, X_val_b, _, _ = train_test_split(\n",
    "    X_binary, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "trainer = TrainModel()\n",
    "\n",
    "trainer.train_models(X_train_c, X_train_b, y_train)\n",
    "\n",
    "final_report = trainer.evaluate_models(X_val_c, X_val_b, y_val)\n",
    "\n",
    "display(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; line-height: 1.8;\">\n",
    "    <h3 style=\"margin-top: 15px;\">Ø¨Ø±Ø±Ø³ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ</h3>\n",
    "</div>\n",
    "\n",
    "<table style=\"width: 100%; text-align: center; margin: 25px 0;\">\n",
    "    <tr>\n",
    "        <td><img src=\"./Assets/BernoulliNB.png\" width=\"800\"></td>\n",
    "        <td><img src=\"./Assets/MultinomialNB_res.png\" width=\"800\"></td>\n",
    "        <td><img src=\"./Assets/DecisionTree.png\" width=\"800\"></td>\n",
    "        <td><img src=\"./Assets/part2_final_res.png\" width=\"800\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Naive bayes with binary vector</td>\n",
    "        <td>Naive bayes with counting vector</td>\n",
    "        <td>Destition tree with depth = 4</td>\n",
    "        <td>final result</td>â€\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; line-height: 1.9; font-size: 1.05em;\">\n",
    "\n",
    "<p>\n",
    "Ù…Ø¯Ù„ Bernoulli Naive Bayes ØªÙˆØ§Ù†Ø³ØªÙ‡ Ø§Ø³Øª ØªÙ…Ø§Ù…ÛŒ Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø§ÛŒ Ù…Ù‡Ù… Ø±Ø§ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ Ùˆ Ø§Ø² Ø§ÛŒÙ† Ù†Ø¸Ø± Recall Ú©Ø§Ù…Ù„ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ù…Ø§ Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ØŒ Ù†Ø±Ø® Ù†Ø³Ø¨ØªØ§ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø§Ø² Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø§ÛŒ Ø§Ø³Ù¾Ù… Ø±Ø§ Ù†ÛŒØ² Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ø¯Ø± Ø¯Ø³ØªÙ‡ Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø§ÛŒ Ù…Ù‡Ù… Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø¯Ù„ÛŒÙ„ Ø§ØµÙ„ÛŒ Ø§ÛŒÙ† Ø±ÙØªØ§Ø± Ø¢Ù† Ø§Ø³Øª Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø¯Ù„ ØµØ±ÙØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ¬ÙˆØ¯ ÛŒØ§ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ÙˆØ§Ú˜Ù‡ Ù‡Ø§ Ø¯Ø± Ù…ØªÙ† Ø§ÛŒÙ…ÛŒÙ„ ØªØµÙ…ÛŒÙ… Ú¯ÛŒØ±ÛŒ Ù…ÛŒ Ú©Ù†Ø¯ Ùˆ Ù‡ÛŒÚ† ØªÙˆØ¬Ù‡ÛŒ Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± Ø¢Ù† Ù‡Ø§ Ù†Ø¯Ø§Ø±Ø¯. Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ØŒ ÙˆØ¬ÙˆØ¯ Ú†Ù†Ø¯ ÙˆØ§Ú˜Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯Ø± ÛŒÚ© Ø§ÛŒÙ…ÛŒÙ„ Ø§Ø³Ù¾Ù… Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ø¨Ø§Ø¹Ø« Ø´ÙˆØ¯ Ù…Ø¯Ù„ Ø¢Ù† Ø§ÛŒÙ…ÛŒÙ„ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø§ÛŒÙ…ÛŒÙ„ Ù…Ù‡Ù… ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ØŒ Ø­ØªÛŒ Ø§Ú¯Ø± Ø§ÛŒÙ† ÙˆØ§Ú˜Ù‡ Ù‡Ø§ ØªÙ†Ù‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ø·Ø­ÛŒ Ø¯Ø± Ù…ØªÙ† Ø¸Ø§Ù‡Ø± Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…Ø¯Ù„ Ø¯Ø±Ø®Øª ØªØµÙ…ÛŒÙ… Ù†ÛŒØ² Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù… Ø±ÛŒØ®ØªÚ¯ÛŒ Ùˆ Ù…ØªØ±ÛŒÚ© Ù‡Ø§ÛŒ Ø¨Ù‡ Ø¯Ø³Øª Ø¢Ù…Ø¯Ù‡ Ù…ÛŒ ØªÙˆØ§Ù† Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø±Ø¯ Ú©Ù‡ Ø±ÙØªØ§Ø± Ø¢Ù† ØªØ§ Ø­Ø¯ Ø²ÛŒØ§Ø¯ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ù…Ø¯Ù„ Bernoulli Naive Bayes Ø§Ø³Øª. Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù†ÛŒØ² Ø¨ÛŒØ´ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ¬ÙˆØ¯ Ø¨Ø±Ø®ÛŒ ÙˆØ§Ú˜Ù‡ Ù‡Ø§ÛŒ Ø®Ø§Øµ Ø§Ù‚Ø¯Ø§Ù… Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø§ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ø¯Ú†Ø§Ø± Ø¨Ø§ÛŒØ§Ø³ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø§ÛŒÙ† ÙˆØ§Ú˜Ù‡ Ù‡Ø§ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø§Ú¯Ø±Ú†Ù‡ Ø¯Ø±Ø®Øª ØªØµÙ…ÛŒÙ… Ø§Ø² Ù†Ø¸Ø± ØªÙØ³ÛŒØ±Ù¾Ø°ÛŒØ±ÛŒ Ù…Ø²ÛŒØª Ø¯Ø§Ø±Ø¯ Ùˆ Ù…ÛŒ ØªÙˆØ§Ù†Ø¯ Ù‚ÙˆØ§Ù†ÛŒÙ†ÛŒ Ø³Ø§Ø¯Ù‡ Ø§Ø² Ø¬Ù†Ø³ ÙˆØ¬ÙˆØ¯ ÛŒØ§ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø¨Ø±Ø®ÛŒ Ú©Ù„Ù…Ø§Øª Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯ØŒ Ø§Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø³Ø¦Ù„Ù‡ Ø®Ø§Øµ ØªÙˆØ§Ù† ØªØ¹Ù…ÛŒÙ… Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø§ÛŒ Ø§Ø³Ù¾Ù… Ù†Ø¯Ø§Ø´ØªÙ‡ Ùˆ Ø¯Ø± Ù†ØªÛŒØ¬Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù„ÛŒ Ø¶Ø¹ÛŒÙ ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø³Ø§ÛŒØ± Ù…Ø¯Ù„ Ù‡Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Ø¯Ø± Ù…Ù‚Ø§Ø¨Ù„ØŒ Ù…Ø¯Ù„ Multinomial Naive Bayes Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø¯Ø± Ù…ÛŒØ§Ù† Ø³Ù‡ Ù…Ø¯Ù„ Ù…ÙˆØ±Ø¯ Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± ØªØ´Ø®ÛŒØµ ØµØ­ÛŒØ­ ØªÙ…Ø§Ù…ÛŒ Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø§ÛŒ Ù…Ù‡Ù…ØŒ ØªÙˆØ§Ù†Ø³ØªÙ‡ Ø§Ø³Øª ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø³ÛŒØ§Ø± Ú©Ù…ØªØ±ÛŒ Ø§Ø² Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø§ÛŒ Ø§Ø³Ù¾Ù… Ø±Ø§ Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ù…Ù‡Ù… ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯. Ø¹Ù„Øª Ø§ØµÙ„ÛŒ Ø§ÛŒÙ† Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ØŒ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† ÙØ±Ú©Ø§Ù†Ø³ ÙˆÙ‚ÙˆØ¹ Ù‡Ø± ÙˆØ§Ú˜Ù‡ Ø¯Ø± Ù…ØªÙ† Ø§ÛŒÙ…ÛŒÙ„ Ø§Ø³Øª. Ø¯Ø± Ø§ÛŒÙ† Ù†Ù…Ø§ÛŒØ´ØŒ Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹Ø§ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ú©Ù‡ Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø´Ø§Ù…Ù„ ØªÚ©Ø±Ø§Ø± Ú†Ù†Ø¯ÛŒÙ† Ø¨Ø§Ø±Ù‡ ÙˆØ§Ú˜Ù‡ Ù‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¯Ø±Ø³ØŒ ØªÙ…Ø±ÛŒÙ† Ùˆ Ø§Ù…ØªØ­Ø§Ù† Ù‡Ø³ØªÙ†Ø¯ØŒ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ø§Ø² Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø§ÛŒ Ø§Ø³Ù¾Ù… Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªÙ†Ù‡Ø§ ÛŒÚ© ÛŒØ§ Ø¯Ùˆ Ø¨Ø§Ø± Ø§Ø² Ø§ÛŒÙ† ÙˆØ§Ú˜Ù‡ Ù‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù†Ø¯ØŒ ØªÙÚ©ÛŒÚ© Ù…ÛŒ Ø´ÙˆÙ†Ø¯.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Ø¨Ù‡ Ø·ÙˆØ± Ú©Ù„ÛŒ Ù†ØªØ§ÛŒØ¬ Ù†Ø´Ø§Ù† Ù…ÛŒ Ø¯Ù‡Ø¯ Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø³Ø¦Ù„Ù‡ØŒ Ù†ÙˆØ¹ Ù†Ù…Ø§ÛŒØ´ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ ØªØ§Ø«ÛŒØ± Ø¨Ø³ÛŒØ§Ø± Ø¨ÛŒØ´ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ù…Ø¯Ù„ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒÚ© Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡ Ù…Ø§Ù†Ù†Ø¯ Naive BayesØŒ Ø¯Ø± Ú©Ù†Ø§Ø± Feature Engineering Ù…Ù†Ø§Ø³Ø¨ØŒ Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ÛŒ Ø¨Ù‡ØªØ± Ø§Ø² Ù…Ø¯Ù„ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ ØªØ± Ù…Ø§Ù†Ù†Ø¯ Ø¯Ø±Ø®Øª ØªØµÙ…ÛŒÙ… Ø´Ø¯Ù‡ Ùˆ Ø§Ù‡Ù…ÛŒØª Ø§Ù†ØªØ®Ø§Ø¨ ØµØ­ÛŒØ­ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø¯Ø± Ù…Ø³Ø§Ø¦Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¨ÛŒ Ù†Ø´Ø§Ù† Ù…ÛŒ Ø¯Ù‡Ø¯.\n",
    "</p>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"part2_submissions\"\n",
    "test_file = \"part2_test.csv\"\n",
    "\n",
    "df_test = pd.read_csv(test_file)\n",
    "df_test['clean_text'] = df_test['message_body'].apply(clean_text)\n",
    "\n",
    "X_test_counts = vectorizer.count_vectorizer.transform(df_test['clean_text'])\n",
    "binary_gen = CountVectorizer(vocabulary=vectorizer.vocab, binary=True)\n",
    "X_test_binary = binary_gen.transform(df_test['clean_text'])\n",
    "\n",
    "trainer.generate_best_submission(\n",
    "    X_test=X_test_counts, \n",
    "    message_texts=df_test['message_body'], \n",
    "    output_folder=output_folder,\n",
    "    model_key='MultinomialNB',\n",
    "    name = \"MultinomialNB_results.csv\"\n",
    ")\n",
    "\n",
    "trainer.generate_best_submission(\n",
    "    X_test=X_test_binary, \n",
    "    message_texts=df_test['message_body'], \n",
    "    output_folder=output_folder,\n",
    "    model_key='BernoulliNB',\n",
    "    name = \"BernoulliNB_results.csv\"\n",
    ")\n",
    "\n",
    "trainer.generate_best_submission(\n",
    "    X_test=X_test_counts, \n",
    "    message_texts=df_test['message_body'], \n",
    "    output_folder=output_folder,\n",
    "    model_key='DecisionTree',\n",
    "    name = \"DecisionTree_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ø³ÙˆØ§Ù„Ø§Øª\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; line-height: 1.9; font-size: 1.05em;\">\n",
    "\n",
    "<p>\n",
    "Ø¯Ø± Ù†Ù…Ø§ÛŒØ´ bag-of-wordsØŒ Ù‡Ø± Ø§ÛŒÙ…ÛŒÙ„ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ Ùˆ ØªØ±ØªÛŒØ¨ Ùˆ Ø³Ø§Ø®ØªØ§Ø± Ø¬Ù…Ù„Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø¯Ø±Ù†ØªÛŒØ¬Ù‡ Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø² ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ù…ØªÙ† Ø·Ø¨ÛŒØ¹ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ØŒ Ø­Ø°Ù Ø´Ø¯Ù‡ Ùˆ ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ù…Ø³ØªÙ‚Ù„ Ø§Ø² Ù…Ø­ØªÙˆØ§ Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø¨ÙˆØ¯.Ø§Ú¯Ø±Ú†Ù‡ Ø§ÛŒÙ† ÙØ±Ø¶ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†ÛŒØ³ØªØŒ Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø³Ø¦Ù„Ù‡â€ŒØ§ÛŒ Ù…Ø§Ù†Ù†Ø¯ ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ…ÛŒÙ„ Ù…Ù‡Ù… Ø§Ø² Ø§Ø³Ù¾Ù…ØŒ Ø§ÛŒÙ† Ø³Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ØªØ§ Ø­Ø¯ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ù…Ù†Ø·Ù‚ÛŒ Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒâ€ŒØ±Ø³Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ø¢Ø²Ù…Ø§ÛŒØ´ØŒ Ù…Ø¯Ù„ Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ Ø­Ø¶ÙˆØ± ÛŒØ§ ÙØ±Ø§ÙˆØ§Ù†ÛŒ ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ ÙØ¶Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ ÙˆØ§Ú©Ù†Ø´ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ Ù‡Ù…ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ø§Ø¹Ø« Ø´Ø¯Ù‡ Naive Bayes Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±ÙˆØ§Ø¨Ø· Ù¾ÛŒÚ†ÛŒØ¯Ù‡ØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "</p>\n",
    "\n",
    "<p>ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ examØŒ deadline Ùˆ homework Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù‡Ù…â€ŒØ²Ù…Ø§Ù† Ø¯Ø± Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ù…Ø³ØªÙ‚Ù„ Ø§Ø² ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ³ØªÙ†Ø¯ Ú©Ù‡ ÙØ±Ø¶ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§Ø±Ø§ Ù†Ù‚Ø¶ Ù…ÛŒ Ú©Ù†Ù†Ø¯ Ø§Ù…Ø§ Ø¯Ø± Naive Bayes Ø§ÛŒÙ† Ù†Ù‚Ø¶ Ù„Ø²ÙˆÙ…Ø§ Ø¨Ù‡ Ø§ÙØª Ø´Ø¯ÛŒØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ù†Ø¬Ø± Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù…Ø¯Ù„ Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªØ§Ø«ÛŒØ± Ø§ÛŒÙ† ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨ÛŒØ´ Ø§Ø² Ù…Ù‚Ø¯Ø§Ø± ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø±Ø¢ÙˆØ±Ø¯ Ú©Ù†Ø¯ØŒ Ø§Ù…Ø§ Ø·Ø¨Ù‚ Ù†ØªÛŒØ¬Ù‡ Ø­Ø§ØµÙ„ Ø´Ø¯Ù‡ Ù„ÛŒØ¨Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± Ø§ØºÙ„Ø¨ Ù…ÙˆØ§Ø±Ø¯ Ø¯Ø±Ø³Øª Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø±Ø¯ Ú©Ù‡ Ø¯Ø±Ø®Øª ØªØµÙ…ÛŒÙ… ØªÙˆØ§Ù†Ø³ØªÙ‡ Ø§Ø³Øª Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙˆØ¬ÙˆØ¯ Ù‡Ù…â€ŒØ²Ù…Ø§Ù† Ú†Ù†Ø¯ ÙˆØ§Ú˜Ù‡ Ø®Ø§Øµ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯ Ùˆ ØªØµÙ…ÛŒÙ…â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ù† Ø¬Ù‡Øª Ø¯Ù‡ÛŒ Ú©Ù†Ø¯ Ø§Ù…Ø§ Ø¨Ø±Ø®ÛŒ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ù… Ø±Ø§ Ù†ÛŒØ² ØµØ±ÙØ§ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¯Ø§Ø´ØªÙ† Ú†Ù†Ø¯ ÙˆØ§Ú˜Ù‡ Ù…Ø´ØªØ±Ú© Ø¨Ø§ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒØŒ Ø¯Ø± Ø¯Ø³ØªÙ‡ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ØŒ Naive Bayes Ø¨Ù‡â€ŒÙˆÛŒÚ˜Ù‡ Ø¯Ø± Ø­Ø§Ù„Øª Ø´Ù…Ø§Ø±Ø´ÛŒØŒ Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† ÙØ±Ø§ÙˆØ§Ù†ÛŒ ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ ØªÙˆØ§Ù†Ø³ØªÙ‡ Ø§Ø³Øª ØªÙÚ©ÛŒÚ© Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ÛŒ Ø¨ÛŒÙ† Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ùˆ Ø§Ø³Ù¾Ù… Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†Ø¯ Ùˆ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡Ø¯.\n",
    "</p>\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
